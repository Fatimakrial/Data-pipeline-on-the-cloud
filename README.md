âš¡ GANS Mobility Data Pipeline
Overview

GANS Mobility is an e-scooter startup using data to optimize fleet distribution.
This project builds a cloud-based ETL pipeline that collects and processes external data â€” including weather, flights, and demographics â€” to support smarter scooter placement decisions.

The system runs automatically on Google Cloud Platform (GCP) and stores all processed data in a MySQL database.

ğŸ§­ Pipeline Summary
APIs & Web Sources
     â†“
Data Extraction (Python)
     â†“
Cleaning & Transformation (pandas)
     â†“
Cloud Storage (MySQL on GCP)
     â†“
Automation (Cloud Functions + Scheduler)


The modular structure allows independent testing and smooth deployment to GCP.

â˜ï¸ Why Cloud?

Automation: Regular data updates via Cloud Scheduler

Scalability: Handles growing data sources

Security: Managed access and credentials

Accessibility: Shared database for analytics and reporting

All functions can be tested locally with Flask before deployment.

ğŸ§° Tools & Libraries
Purpose	Stack
Language	Python 3
Extraction	requests, BeautifulSoup, OpenWeather API
Processing	pandas, json, custom utils
Storage	MySQL (Google Cloud SQL)
Automation	Google Cloud Functions, Cloud Scheduler
Testing	Flask
ğŸ“‚ Project Structure
ğŸ“ gans-pipeline
â”œâ”€â”€ cities.py          # City and country info
â”œâ”€â”€ weather.py         # Weather data
â”œâ”€â”€ flights.py         # Flight activity
â”œâ”€â”€ demographics.py    # Population data
â”œâ”€â”€ utils.py           # Helper functions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Setup
# Clone repo
git clone https://github.com/yourusername/gans-pipeline.git
cd gans-pipeline

# Install dependencies
pip install -r requirements.txt


Update credentials in utils.py:

RAPID_API_KEY = "your_key"
WEATHER_API_KEY = "your_key"
schema = "db_schema"
host = "gcp_host"
password = "password"


Run locally:

python cities.py
python weather.py
python flights.py


Deploy to Google Cloud:

@functions_framework.http
def main(request):
    # ETL process here
    return "Pipeline executed successfully"

ğŸ“˜ Key Takeaways

Built a scalable ETL pipeline using Python and GCP

Combined API and web-scraped data sources

Automated workflows with serverless cloud tools

Developed and tested locally with Flask
